# Semantic Cache for Large Language Models (LLM) ğŸŒâš¡

# scLLM - Structure du Projet ğŸš€

## ğŸ“– Description
scLLM est un projet structurÃ© permettant de sÃ©parer clairement les diffÃ©rentes composantes essentielles :
- **`data/`** ğŸ“Š : Gestion des donnÃ©es (brutes, prÃ©traitÃ©es, prÃ©dictions, etc.).
- **`docs/`** ğŸ“„ : Documentation dÃ©taillÃ©e.
- **`logs/`** ğŸ“ : Logs d'exÃ©cution.
- **`models/`** ğŸ¤– : ModÃ¨les sauvegardÃ©s.
- **`reports/`** ğŸ“ˆ : Rapports d'analyse et rÃ©sultats visuels.
- **`src/`** ğŸ—ï¸ : Code source organisÃ© par fonctionnalitÃ©s (gestion des donnÃ©es, modÃ¨les, visualisation, etc.).
- **`tests/`** âœ… : Tests pour garantir la stabilitÃ© du code.
- **`website/`** ğŸŒ : Interface web pour interagir avec le modÃ¨le (classification, visualisation des rÃ©sultats, etc.).

---

## ğŸ“‚ Arborescence du projet
```plaintext
scLLM
â”œâ”€â”€ README.md
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ clean_raw        # DonnÃ©es nettoyÃ©es mais non transformÃ©es
â”‚   â”œâ”€â”€ metric           # RÃ©sultats des mÃ©triques d'Ã©valuation
â”‚   â”œâ”€â”€ prediction       # PrÃ©dictions gÃ©nÃ©rÃ©es par les modÃ¨les
â”‚   â”œâ”€â”€ processed        # DonnÃ©es prÃ©traitÃ©es prÃªtes pour l'entraÃ®nement
â”‚   â”œâ”€â”€ raw              # DonnÃ©es brutes non traitÃ©es
â”‚   â””â”€â”€ temporary        # Fichiers temporaires gÃ©nÃ©rÃ©s pendant le traitement
â”œâ”€â”€ docs                 # Documentation du projet
â”œâ”€â”€ logs                 # Fichiers de logs gÃ©nÃ©rÃ©s pendant l'exÃ©cution
â”œâ”€â”€ models               # ModÃ¨les entraÃ®nÃ©s et sauvegardÃ©s
â”œâ”€â”€ reports              # Rapports gÃ©nÃ©rÃ©s (visualisation, analyse, etc.)
â”œâ”€â”€ requierements        # DÃ©pendances et configurations requises
â”œâ”€â”€ semantic             # Informations liÃ©es Ã  la sÃ©mantique et aux embeddings
â”œâ”€â”€ src                  # Code source principal du projet
â”‚   â”œâ”€â”€ data             # Scripts de gestion et de transformation des donnÃ©es
â”‚   â”œâ”€â”€ features         # Extraction et ingÃ©nierie des caractÃ©ristiques
â”‚   â”œâ”€â”€ models          # DÃ©finition, entraÃ®nement et Ã©valuation des modÃ¨les
â”‚   â”œâ”€â”€ reports         # GÃ©nÃ©ration de rapports et visualisations
â”‚   â”œâ”€â”€ tools           # Outils auxiliaires et utilitaires
â”‚   â””â”€â”€ visualisation   # Scripts de visualisation des rÃ©sultats
â”œâ”€â”€ tests                # Tests unitaires et d'intÃ©gration
â””â”€â”€ website              # Interface web pour l'exploitation du modÃ¨le
    â””â”€â”€ web_classification
        â”œâ”€â”€ __pycache__  # Fichiers compilÃ©s Python
        â”œâ”€â”€ connexion.csv             # Historique des connexions
        â”œâ”€â”€ highlighted_prompts.json  # Prompts mis en Ã©vidence
        â”œâ”€â”€ login.csv                 # Informations de connexion
        â”œâ”€â”€ main.py                   # Script principal du site web
        â”œâ”€â”€ prompts.csv               # Prompts utilisÃ©s
        â””â”€â”€ static                    # Fichiers statiques (HTML, CSS, JS, images)
            â”œâ”€â”€ aide.html              # Page d'aide
            â”œâ”€â”€ css                    # Fichiers CSS pour le style
            â”‚   â”œâ”€â”€ styles-index.css
            â”‚   â”œâ”€â”€ styles-login.css
            â”‚   â””â”€â”€ styles-stats.css
            â”œâ”€â”€ img                    # Images du site
            â”‚   â””â”€â”€ aide.png
            â”œâ”€â”€ index.html             # Page d'accueil
            â”œâ”€â”€ js                     # Scripts JavaScript pour l'interactivitÃ©
            â”‚   â”œâ”€â”€ charts-index.js
            â”‚   â”œâ”€â”€ charts-stats.js
            â”‚   â”œâ”€â”€ connection-index.js
            â”‚   â”œâ”€â”€ connection-login.js
            â”‚   â”œâ”€â”€ error-login.js
            â”‚   â”œâ”€â”€ highlighting-index.js
            â”‚   â”œâ”€â”€ highlighting-stats.js
            â”‚   â”œâ”€â”€ history-index.js
            â”‚   â”œâ”€â”€ main-index.js
            â”‚   â”œâ”€â”€ main-login.js
            â”‚   â”œâ”€â”€ main-stats.js
            â”‚   â”œâ”€â”€ network-stats.js
            â”‚   â”œâ”€â”€ prompts-index.js
            â”‚   â””â”€â”€ register-login.js
            â”œâ”€â”€ login.html             # Page de connexion
            â””â”€â”€ stats.html             # Page des statistiques
```

---

## ğŸ”§ Installation

Pour installer et utiliser le projet, suivez ces Ã©tapes :

1. Clonez le dÃ©pÃ´t :
   ```bash
   git clone https://github.com/your-username/scLLM.git
   cd scLLM
   ```

2. Installez les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸŒ Lancement du service web

### â–¶ï¸ DÃ©marrer le serveur
```bash
nohup uvicorn main:app --host 0.0.0.0 --port 7000 > output.log 2>&1 &
```

### â¹ï¸ ArrÃªter le serveur
```bash
kill $(pgrep -f "uvicorn main:app")
```

---

## ğŸ“Œ Objectifs
- **Optimiser la gestion des donnÃ©es et des modÃ¨les** ğŸ”„
- **AmÃ©liorer la scalabilitÃ© et la rÃ©utilisation des rÃ©sultats** ğŸš€
- **Faciliter l'intÃ©gration et la visualisation des analyses** ğŸ“Š

Ce README a Ã©tÃ© conÃ§u pour structurer et clarifier l'architecture du projet **scLLM**. N'hÃ©sitez pas Ã  proposer des amÃ©liorations ou des ajouts ! ğŸ’¡


## Overview ğŸš€

The increasing size of large language models (LLMs) has resulted in significant computational costs, making their deployment at scale challenging. This project explores the hypothesis that a semantic cache could reduce these costs by efficiently reusing previously generated responses for semantically similar queries. Unlike traditional caching methods that rely on exact matches, our approach uses vector representations and semantic similarity measures to identify when precomputed responses can be reused. This method aims not only to improve computational efficiency but also to maintain the quality of responses. Future experimental evaluations will be necessary to test this hypothesis and assess the impact of the semantic cache on inference time and the overall performance of LLMs.

## Motivation ğŸ’¡

Large Language Models (LLMs) are deep neural networks pre-trained on vast amounts of textual data. These models are capable of understanding, generating, and transforming text based on the knowledge they have learned. LLMs are widely used in a variety of applications, such as machine translation, text generation, and question answering.

The performance of an LLM largely depends on the formulation of the input prompt, as it influences how the model interprets and generates its output. Therefore, optimizing how prompts are managed, particularly for similar prompts, is key to improving LLM performance.

In this project, we hypothesize that a semantic cache could efficiently manage similar prompts, improving the overall computational efficiency of LLMs without compromising the quality of the responses.

## Key Concepts ğŸ”‘

### Prompts ğŸ“
A prompt is the input sequence of text provided to the model to guide its response. It could be a question, instruction, or context. The way a prompt is structured can greatly influence the model's understanding and output. For example:

- **Example Prompt**: "What is the weather in Paris?"
  
The performance of an LLM is highly dependent on how the prompt is formulated, as it determines how the model interprets the query and generates a response.

### Semantic Caching ğŸ’¾
Semantic caching is the process of reusing responses that are generated for semantically similar queries. Instead of matching exact strings (as in traditional caching), semantic caching uses vector embeddings of the prompts and calculates semantic similarity to identify opportunities for reusing responses.

### Request vs. Precision ğŸ¯
In our approach, we distinguish two main components in a prompt: **Request** and **Precision**.

- **Request**: This is the core question or instruction being asked to the model. It is the part that directly queries the model to get an answer. For example, in the prompt "What is the weather in Paris today?", the request is "What is the weather?".
  
- **Precision**: This refers to additional information that helps to refine or specify the request. It helps make the answer more accurate. In the same example, "in Paris today" represents the precision that contextualizes the request.

Understanding this distinction helps us design the semantic cache, which can effectively identify when the same request (but with different precisions) has been answered previously, allowing the system to reuse answers efficiently.

## Approach ğŸ› ï¸

The core idea is to use semantic representations (such as vector embeddings from transformer models like BERT or GPT) to encode prompts. When a new prompt is received, we compute its vector representation and compare it to those stored in the cache using a similarity measure like cosine similarity.

If a sufficiently similar prompt exists in the cache, the system can retrieve and return the precomputed response. If no similar prompt is found, the system generates a new response and stores both the prompt and response for future reuse.

### Key Components ğŸ”§
1. **Vectorization of Prompts**: The input prompts are transformed into vector embeddings using a pre-trained model (e.g., BERT or GPT).
2. **Similarity Calculation**: For every incoming prompt, we calculate its similarity with previously cached prompts using a similarity metric (e.g., cosine similarity).
3. **Caching Mechanism**: If the similarity between the new prompt and an existing one exceeds a predefined threshold, the cached response is reused. Otherwise, the model generates a new response and stores it.
4. **Precision Handling**: The system distinguishes between the request and the precision of the prompt to handle subtle variations in context, ensuring that reusing cached responses doesnâ€™t lead to irrelevant or inaccurate answers.

## Objectives ğŸ¯
- **Improve Computational Efficiency**: By reusing previously generated responses, we can reduce the computational overhead associated with generating responses for similar prompts.
- **Maintain Response Quality**: Ensure that the reused responses are still relevant and accurate by considering the precision in each prompt.
- **Reduce Inference Time**: Decrease the amount of time spent generating responses by leveraging a cache of precomputed answers.


